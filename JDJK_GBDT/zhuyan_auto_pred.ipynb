{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import (\n",
    "    GBTClassificationModel,\n",
    "    RandomForestClassificationModel,\n",
    "    LogisticRegressionModel,\n",
    ")\n",
    "import pyspark.ml.evaluation as evals\n",
    "import pyspark.ml.tuning as tune\n",
    "import subprocess \n",
    "from functools import reduce\n",
    "from pyspark.sql.types import DoubleType "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://clientnode2.jinxintian.com:4067\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>zhy_autopred</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb6df832c88>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "application_name = \"zhy_autopred\"\n",
    "\n",
    "spark.stop()\n",
    "spark = (\n",
    "    SparkSession.builder.appName(application_name)\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"200\")\n",
    "    .config(\"spark.default.parallelism\", \"1500\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"1500\")\n",
    "    .config(\"spark.executor.memoryOverhead\", \"10G\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_prediction:\n",
    "    def __init__(self, model_name, model_type):\n",
    "        self.model_name = model_name\n",
    "        self.model_type = model_type\n",
    "        self.operator = self.model_name.split(\"_\")[0]\n",
    "        self.n_dpi_days = 15\n",
    "        self.model_basic_message = spark.read.parquet(\n",
    "            f\"/user/zhuyan/model/{self.model_name}/model_basic_message\"\n",
    "        )\n",
    "        self.dpi_symbol = \"8_dpi_result\" if self.operator == \"cmcc\" else \"9_dpi_result\"\n",
    "        self.operator_id = {\"ctcc\": 0, \"cmcc\": 1, \"cucc\": 2}.get(self.operator)\n",
    "        self.df_message = self.model_basic_message.toPandas()\n",
    "        self.n_visited, self.n_sent, self.n_sent_bt, self.n_host = (\n",
    "            f\"e_n_visited_{self.dpi_symbol}_modelling\",\n",
    "            f\"e_n_sent_{self.operator}_modelling\",\n",
    "            f\"e_n_sent_bt_{self.operator}_modelling\",\n",
    "            f\"e_n_host_{self.dpi_symbol}_modelling\",\n",
    "        )\n",
    "        self.fake_data = spark.read.parquet(\n",
    "            f\"/user/zhuyan/model/{self.model_name}/fake_data\"\n",
    "        )\n",
    "        self.pipeline_model = PipelineModel.load(\n",
    "            f\"/user/zhuyan/model/{self.model_name}/pipeline_model\"\n",
    "        )\n",
    "        if self.model_type == \"gbt\":\n",
    "            self.model = GBTClassificationModel.load(\n",
    "                f\"/user/zhuyan/model/{self.model_name}/{self.model_type}\"\n",
    "            )\n",
    "        elif self.model_type == \"rdf\":\n",
    "            self.model = RandomForestClassificationModel.load(\n",
    "                f\"/user/zhuyan/model/{self.model_name}/{self.model_type}\"\n",
    "            )\n",
    "        else:\n",
    "            self.model = LogisticRegressionModel.load(\n",
    "                f\"/user/zhuyan/model/{self.model_name}/{self.model_type}\"\n",
    "            )\n",
    "        self.hosts = str(\n",
    "            tuple(self.df_message.hosts.loc[lambda x: x != \"null_value\"].tolist())\n",
    "        )\n",
    "        self.products_click = str(\n",
    "            tuple(\n",
    "                self.df_message.products_click.loc[lambda x: x != \"null_value\"].tolist()\n",
    "            )\n",
    "        )\n",
    "        self.product_sent = str(\n",
    "            tuple(\n",
    "                self.df_message.products_send.loc[lambda x: x != \"null_value\"].tolist()\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_people_for_pred(self):\n",
    "        spark.sql(\n",
    "            f\"\"\"\n",
    "        select all.rysecret\n",
    "        from (  select phone_number rysecret, myq.host\n",
    "                from etl_fetch.{self.dpi_symbol}\n",
    "                lateral view explode(hosts) q as myq\n",
    "                where p_biz in ('jrunion')\n",
    "                    AND p_date >= date_format(date_add(current_date(), -40), 'yyyyMMdd')\n",
    "                    AND myq.host IN {self.hosts}\n",
    "              union all\n",
    "              select distinct rysecret, 1 host\n",
    "                from etl_swap.rp_biz_access_log\n",
    "                where p_biz in ('loan', 'credit', 'insurance') \n",
    "                and dt >= date_sub(current_date(), 180)\n",
    "            ) all\n",
    "        join (select uid rysecret\n",
    "                from dw_resources.mapping_uid_property\n",
    "                where p_operate = {self.operator_id}\n",
    "                and p_province not in ('11')\n",
    "                ) pp on all.rysecret = pp.rysecret\n",
    "        GROUP BY all.rysecret\n",
    "        \"\"\"\n",
    "        ).cache().createOrReplaceTempView(\"people_for_prediction\")\n",
    "\n",
    "    def get_dummy_data(self, df, df_message, df_col, df_message_col, label=False):\n",
    "        \"\"\"\n",
    "        df_col, df_message_colä¸ºlist\n",
    "        \"\"\"\n",
    "        rename_dic = dict(zip(df_message_col, df_col))\n",
    "        fillna_dic = {\"rysecret\": \"A\", \"sent_date\": \"22220202\"}\n",
    "        if label:\n",
    "            fillna_dic[\"label\"] = 0\n",
    "        dummy_data = (\n",
    "            pd.concat(\n",
    "                [\n",
    "                    pd.DataFrame(columns=df.columns),\n",
    "                    (\n",
    "                        df_message[df_message_col]\n",
    "                        .replace(\"null_value\", np.nan)\n",
    "                        .dropna(how=\"all\")\n",
    "                        .fillna(method=\"ffill\")\n",
    "                        .rename(columns=rename_dic)\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            .fillna(fillna_dic)\n",
    "            .fillna(-999)\n",
    "        )\n",
    "        return dummy_data\n",
    "\n",
    "    def deal_with_data(self):\n",
    "        label_data = spark.sql(\n",
    "            f\"\"\"\n",
    "        SELECT \n",
    "\n",
    "            all.rysecret, \n",
    "            n_ins_host_10, n_ins_host_20, n_ins_host_30, fre_ins_host_10, fre_ins_host_20, fre_ins_host_30,\n",
    "            n_loan_host_10, n_loan_host_20, n_loan_host_30, fre_loan_host_10, fre_loan_host_20, fre_loan_host_30,\n",
    "            n_credit_host_10, n_credit_host_20, n_credit_host_30, fre_credit_host_10, fre_credit_host_20, fre_credit_host_30,\n",
    "            n_other_host_10, n_other_host_20, n_other_host_30,  fre_other_host_10, fre_other_host_20, fre_other_host_30,\n",
    "            ins_host_rate_10, ins_host_rate_20, ins_host_rate_30, ins_fre_rate_10, ins_fre_rate_20, ins_fre_rate_30,\n",
    "            loan_host_rate_10, loan_host_rate_20, loan_host_rate_30, loan_fre_rate_10, loan_fre_rate_20, loan_fre_rate_30,\n",
    "            credit_host_rate_10, credit_host_rate_20, credit_host_rate_30, credit_fre_rate_10, credit_fre_rate_20, credit_fre_rate_30,\n",
    "            n_ins_host_avg_30, n_ins_host_sd_30, n_ins_host_cv_30,\n",
    "            n_loan_host_avg_30, n_loan_host_sd_30, n_loan_host_cv_30,\n",
    "            n_credit_host_avg_30, n_credit_host_sd_30, n_credit_host_cv_30,\n",
    "            n_other_host_avg_30, n_other_host_sd_30, n_other_host_cv_30,\n",
    "            ins_host_rate_avg_30, ins_host_rate_sd_30, ins_host_rate_cv_30,\n",
    "            loan_host_rate_avg_30, loan_host_rate_sd_30, loan_host_rate_cv_30,\n",
    "            credit_host_rate_avg_30, credit_host_rate_sd_30, credit_host_rate_cv_30,\n",
    "            fre_ins_host_avg_30, fre_ins_host_sd_30, fre_ins_host_cv_30,\n",
    "            fre_loan_host_avg_30, fre_loan_host_sd_30, fre_loan_host_cv_30,\n",
    "            fre_credit_host_avg_30, fre_credit_host_sd_30, fre_credit_host_cv_30,\n",
    "            fre_other_host_avg_30, fre_other_host_sd_30, fre_other_host_cv_30,\n",
    "            ins_fre_rate_avg_30, ins_fre_rate_sd_30, ins_fre_rate_cv_30,\n",
    "            loan_fre_rate_avg_30, loan_fre_rate_sd_30, loan_fre_rate_cv_30,\n",
    "            credit_fre_rate_avg_30, credit_fre_rate_sd_30, credit_fre_rate_cv_30,\n",
    "            sent_30, sent_90, sent_180, days_since_sent_date,\n",
    "            click_30, click_90, click_180, days_since_click_date,\n",
    "            myl_rank, price, age, gender, maker, brand, new_age,\n",
    "            rank.city_code, province_code, city_level \n",
    "\n",
    "        FROM people_for_prediction all\n",
    "        LEFT JOIN (\n",
    "                SELECT *\n",
    "                FROM bigdata_insurance.{self.n_host}\n",
    "                WHERE update_date >= date_format(date_add(current_date(), -{self.n_dpi_days}), 'yyyyMMdd')\n",
    "                AND update_date < date_format(date_add(current_date(), -0), 'yyyyMMdd')\n",
    "                ) n_host on all.rysecret = n_host.rysecret\n",
    "        LEFT JOIN (\n",
    "                SELECT \n",
    "                    rysecret, update_date, sent_30, sent_90, sent_180, datediff(update_dt, sent_date) days_since_sent_date,\n",
    "                    click_30, click_90, click_180, datediff(update_dt, click_date) days_since_click_date\n",
    "                FROM (\n",
    "                    SELECT *, from_unixtime(unix_timestamp(cast(update_date as string), 'yyyyMMdd'), 'yyyy-MM-dd') update_dt\n",
    "                    FROM bigdata_insurance.{self.n_sent}\n",
    "                    ) sent    \n",
    "                WHERE update_date >= date_format(date_add(current_date(), -{self.n_dpi_days}), 'yyyyMMdd')\n",
    "                    AND update_date < date_format(date_add(current_date(), -0), 'yyyyMMdd')\n",
    "                ) n_sent on all.rysecret = n_sent.rysecret\n",
    "        LEFT JOIN (\n",
    "                SELECT rysecret, age, gender, price, maker\n",
    "                FROM bigdata_insurance.e_static_feature_table\n",
    "                ) st on all.rysecret = st.rysecret\n",
    "        LEFT JOIN (\n",
    "                SELECT rysecret, brand\n",
    "                FROM bigdata_insurance.e_brand_table\n",
    "                ) bt on all.rysecret = bt.rysecret\n",
    "        LEFT JOIN (\n",
    "                SELECT rysecret, first(cast(rank as int)) myl_rank\n",
    "                FROM (\n",
    "                    SELECT uid rysecret, rank, p_date, max(p_date) over (partition by uid) max_date\n",
    "                    FROM sample.e_mayilian) sample\n",
    "                WHERE p_date = max_date\n",
    "                GROUP BY rysecret\n",
    "                ) myl on all.rysecret = myl.rysecret\n",
    "        LEFT JOIN (\n",
    "                SELECT uid rysecret, first(age) new_age\n",
    "                FROM model_dig.e_user_age_col\n",
    "                WHERE age is not null\n",
    "                GROUP BY rysecret\n",
    "                ) age on all.rysecret = age.rysecret\n",
    "        JOIN (\n",
    "                SELECT uid rysecret, citycode city_code\n",
    "                FROM dw_resources.mapping_uid_property\n",
    "                ) city on all.rysecret = city.rysecret\n",
    "        JOIN (\n",
    "                SELECT city_id city_code, province_id province_code, city_level_id city_level \n",
    "                FROM bigdata_insurance.e_citycode_rank_dict\n",
    "                ) rank on city.city_code = rank.city_code\n",
    "        \"\"\"\n",
    "        ).cache()\n",
    "        dpi_data = spark.sql(\n",
    "            f\"\"\"\n",
    "        SELECT \n",
    "            all.rysecret, n_visited.host,\n",
    "            n_10, n_20, n_30, n_avg_30, n_cv_30,\n",
    "            fre_10, fre_20, fre_30, fre_avg_30, fre_cv_30\n",
    "        FROM people_for_prediction all\n",
    "        JOIN (\n",
    "            SELECT *\n",
    "            FROM bigdata_insurance.{self.n_visited}\n",
    "            WHERE update_date >= date_format(date_add(current_date(), -{self.n_dpi_days}), 'yyyyMMdd')\n",
    "                AND update_date < date_format(date_add(current_date(), -0), 'yyyyMMdd') \n",
    "                AND host IN {self.hosts}\n",
    "            ) n_visited ON all.rysecret = n_visited.rysecret \n",
    "        \"\"\"\n",
    "        ).cache()\n",
    "\n",
    "        click_product_data = spark.sql(\n",
    "            f\"\"\"\n",
    "        SELECT \n",
    "            all.rysecret, n_sent_bt.product,\n",
    "            click_30, click_90, click_180, click_360, fre_click_30, fre_click_90, fre_click_180, fre_click_360, days_since_click_date,\n",
    "            e_ip_30, e_ip_90, e_ip_180, e_ip_360, fre_e_ip_30, fre_e_ip_90, fre_e_ip_180, fre_e_ip_360, days_since_e_ip_date\n",
    "        FROM people_for_prediction all\n",
    "        JOIN (\n",
    "            SELECT *\n",
    "            FROM bigdata_insurance.{self.n_sent_bt}\n",
    "            WHERE update_date >= date_format(date_add(current_date(), -{self.n_dpi_days}), 'yyyyMMdd')\n",
    "                AND update_date < date_format(date_add(current_date(), -0), 'yyyyMMdd')\n",
    "                AND product IN {self.products_click} \n",
    "            ) n_sent_bt ON all.rysecret = n_sent_bt.rysecret\n",
    "        \"\"\"\n",
    "        ).cache()\n",
    "\n",
    "        sent_product_data = spark.sql(\n",
    "            f\"\"\"\n",
    "        SELECT \n",
    "            all.rysecret,  n_sent_bt.product,\n",
    "            sent_30, sent_90, sent_180, sent_360, days_since_sent_date,\n",
    "            call_30, call_90, call_180, call_360, days_since_call_date,\n",
    "            pick_30, pick_90, pick_180, pick_360, days_since_pick_date\n",
    "        FROM people_for_prediction all\n",
    "        JOIN (\n",
    "            SELECT *\n",
    "            FROM bigdata_insurance.{self.n_sent_bt}\n",
    "            WHERE update_date >= date_format(date_add(current_date(), -{self.n_dpi_days}), 'yyyyMMdd')\n",
    "                AND update_date < date_format(date_add(current_date(), -0), 'yyyyMMdd') \n",
    "                AND product IN {self.product_sent}\n",
    "            ) n_sent_bt ON all.rysecret = n_sent_bt.rysecret \n",
    "        \"\"\"\n",
    "        ).cache()\n",
    "\n",
    "        label_data = (\n",
    "            label_data.fillna(\n",
    "                {\n",
    "                    \"maker\": \"others\",\n",
    "                    \"brand\": \"others\",\n",
    "                    \"price\": -999,\n",
    "                    \"age\": 0,\n",
    "                    \"gender\": 2,\n",
    "                    \"new_age\": \"null\",\n",
    "                    \"myl_rank\": -999,\n",
    "                }\n",
    "            )\n",
    "            .fillna(\n",
    "                -999,\n",
    "                subset=[\n",
    "                    ele\n",
    "                    for ele in label_data.columns\n",
    "                    if (\"_cv_\" in ele) or (\"days_since_\" in ele)\n",
    "                ],\n",
    "            )\n",
    "            .fillna(0)\n",
    "        )\n",
    "\n",
    "        dummy_dpi_data = self.get_dummy_data(\n",
    "            dpi_data, self.df_message, [\"host\"], [\"hosts\"]\n",
    "        )\n",
    "        dpi_data = (\n",
    "            spark.createDataFrame(dummy_dpi_data)\n",
    "            .union(dpi_data)\n",
    "            .filter(F.col(\"rysecret\") != \"A\")\n",
    "            .groupBy([\"rysecret\"])\n",
    "            .pivot(\"host\")\n",
    "            .agg(\n",
    "                *(\n",
    "                    F.first(i).alias(i)\n",
    "                    for i in dpi_data.columns\n",
    "                    if i not in [\"rysecret\", \"host\"]\n",
    "                )\n",
    "            )\n",
    "            .fillna(0)\n",
    "        )\n",
    "        print(\"dpi_data\")\n",
    "\n",
    "        dummy_click_product_data = self.get_dummy_data(\n",
    "            click_product_data, self.df_message, [\"product\"], [\"products_click\"]\n",
    "        )\n",
    "        click_product_data = (\n",
    "            spark.createDataFrame(dummy_click_product_data)\n",
    "            .union(click_product_data)\n",
    "            .filter(F.col(\"rysecret\") != \"A\")\n",
    "            .groupBy([\"rysecret\"])\n",
    "            .pivot(\"product\")\n",
    "            .agg(\n",
    "                *(\n",
    "                    F.first(i).alias(i)\n",
    "                    for i in click_product_data.columns\n",
    "                    if i not in [\"rysecret\", \"product\"]\n",
    "                )\n",
    "            )\n",
    "            .fillna(0)\n",
    "        )\n",
    "        print(\"click_data\")\n",
    "\n",
    "        dummy_sent_product_data = self.get_dummy_data(\n",
    "            sent_product_data, self.df_message, [\"product\"], [\"products_send\"]\n",
    "        )\n",
    "        sent_product_data = (\n",
    "            spark.createDataFrame(dummy_sent_product_data)\n",
    "            .union(sent_product_data)\n",
    "            .filter(F.col(\"rysecret\") != \"A\")\n",
    "            .groupBy([\"rysecret\"])\n",
    "            .pivot(\"product\")\n",
    "            .agg(\n",
    "                *(\n",
    "                    F.first(i).alias(i)\n",
    "                    for i in sent_product_data.columns\n",
    "                    if i not in [\"rysecret\", \"product\"]\n",
    "                )\n",
    "            )\n",
    "            .fillna(0)\n",
    "        )\n",
    "        print(\"sent_data\")\n",
    "\n",
    "        all_data = (\n",
    "            reduce(\n",
    "                lambda df1, df2: df1.join(df2, on=[\"rysecret\"], how=\"left\"),\n",
    "                [label_data, dpi_data, click_product_data, sent_product_data],\n",
    "            )\n",
    "            .withColumn(\"sent_date\", F.lit(\"20750101\"))\n",
    "            .withColumn(\"label\", F.lit(0))\n",
    "        )\n",
    "        print(\"all_data\")\n",
    "        for col in [i for i in self.fake_data.columns if i not in all_data.columns]:\n",
    "            all_data = all_data.withColumn(col, F.lit(0))\n",
    "\n",
    "        all_data = (\n",
    "            all_data.select(self.fake_data.columns)\n",
    "            .fillna(-999)\n",
    "            .dropDuplicates(subset=[\"rysecret\"])\n",
    "        )\n",
    "        return all_data\n",
    "\n",
    "    def data_deal_for_train(self, all_data):\n",
    "        piped_all_data = self.pipeline_model.transform(all_data)\n",
    "\n",
    "        data_pred = self.model.transform(piped_all_data)\n",
    "\n",
    "        data_pred = data_pred.select(\"rysecret\", \"probability\", \"prediction\")\n",
    "\n",
    "        data_pred.cache()\n",
    "        vec_to_columns = F.udf(lambda x: x[1].item(), DoubleType())\n",
    "        data_pred1 = data_pred.withColumn(\"p_prob\", vec_to_columns(\"probability\"))\n",
    "\n",
    "        operator_id2 = 8 if self.operator == \"cmcc\" else 9\n",
    "\n",
    "        rule_name_ = \"_\".join(\"cmcc_jdjk_click\".split(\"_\")[1:])\n",
    "        rule_name = f\"{self.operator}_dpi_{operator_id2}_dym_{rule_name_}_zhy\"\n",
    "        print(rule_name)\n",
    "\n",
    "        return data_pred1, rule_name\n",
    "\n",
    "    def get_threshold_count(self, test_pre, prob_col=\"p_prob\"):\n",
    "        temp = []\n",
    "        for threshhold in np.arange(0, 1, 0.1):\n",
    "            num_p = test_pre.filter(F.col(prob_col) >= threshhold).count()\n",
    "            temp.append((threshhold.round(2), num_p))\n",
    "        df = pd.DataFrame(temp, columns=[\"prob\", \"count\"])\n",
    "        n_max = df[df[\"count\"] >= 10000].prob.tolist()[-1]\n",
    "        print(n_max)\n",
    "        return n_max\n",
    "\n",
    "    def save_data(self, data_pred, n_max, rule_name):\n",
    "        vec_to_columns = F.udf(lambda x:x[1].item(), DoubleType()) \n",
    "        (\n",
    "            data_pred.withColumn(\"p_prob\", vec_to_columns(\"probability\"))\n",
    "            .filter(F.col(\"p_prob\") >= n_max)\n",
    "            .select(\"rysecret\")\n",
    "            .createOrReplaceTempView(\"shot\")\n",
    "        )\n",
    "        spark.sql(\n",
    "            \"\"\"\n",
    "        set hive.exec.dynamic.partition.mode=nonstrict\n",
    "        \"\"\"\n",
    "        )\n",
    "        spark.sql(\n",
    "            f\"\"\"\n",
    "        INSERT OVERWRITE table bigdata_insurance.e_zhuyan_bx_rules_pre partition(rule_name) \n",
    "        SELECT DISTINCT a.rysecret, b.citycode, b.operator,  b.suffix, b.province, \n",
    "                date_add(current_date(), 1) AS p_date, '{rule_name}' AS rule_name\n",
    "        FROM shot AS a \n",
    "        JOIN dw_resources.mapping_uid_property AS b ON a.rysecret = b.uid \n",
    "        LEFT JOIN (select rysecret \n",
    "                    from bigdata_insurance.e_bt_black_list_bx \n",
    "                    where level = 'jdjk_waihu') AS c ON a.rysecret = c.rysecret \n",
    "        WHERE c.rysecret is NULL AND b.operator = {self.operator_id}\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "    def generate(self):\n",
    "        self.get_people_for_pred()\n",
    "        all_data = self.deal_with_data()\n",
    "        data_pred, rule_name = self.data_deal_for_train(all_data)\n",
    "        n_max = self.get_threshold_count(data_pred)\n",
    "        self.save_data(data_pred, n_max, rule_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "md1 = model_prediction(\"cmcc_jdjk_click\", \"gbt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpi_data\n",
      "click_data\n",
      "sent_data\n",
      "all_data\n",
      "cmcc_dpi_8_dym_jdjk_click_zhy\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "md1.generate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
