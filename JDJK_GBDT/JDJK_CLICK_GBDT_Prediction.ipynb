{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import random as rd\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer,\n",
    "    OneHotEncoder,\n",
    "    OneHotEncoderEstimator,\n",
    "    VectorAssembler,\n",
    ")\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import (\n",
    "    GBTClassifier,\n",
    "    GBTClassificationModel,\n",
    "    RandomForestClassificationModel,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "\n",
    "from pyspark.ml import evaluation as evals\n",
    "from pyspark.ml import tuning as tune\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "operator, product_name_, model_type = (\"cmcc\", \"jdjk\", \"click\")\n",
    "model_name = operator + \"_\" + product_name_ + \"_\" + model_type\n",
    "operator_id = {\"ctcc\": 0, \"cmcc\": 1, \"cucc\": 2}.get(operator)\n",
    "product_name = \"('IYB_JDJKZX_BT')\"\n",
    "print(model_name, product_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"zhy_model_{}_prediction\".format(model_name))\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"120\")\n",
    "    .config(\"spark.executor.memoryOverhead\", \"12G\")\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", \"12G\")\n",
    "    .config(\"spark.driver.allowMultipleContexts\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = spark.read.parquet(f\"/user/zhuyan/model/{model_name}/fake_data\")\n",
    "\n",
    "pipeline_model = PipelineModel.load(f\"/user/zhuyan/model/{model_name}/pipeline_model\")\n",
    "\n",
    "gbt_model = GBTClassificationModel.load(f\"/user/zhuyan/model/{model_name}/gbt\")\n",
    "\n",
    "model_basic_message = spark.read.parquet(\n",
    "    f\"/user/zhuyan/model/{model_name}/model_basic_message\"\n",
    ")\n",
    "df_message = model_basic_message.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpi_symbol = \"8_dpi_result\" if operator == \"cmcc\" else \"9_dpi_result\"\n",
    "n_visited, n_sent, n_sent_bt, n_host = (\n",
    "    f\"e_n_visited_{dpi_symbol}_modelling\",\n",
    "    f\"e_n_sent_{operator}_modelling\",\n",
    "    f\"e_n_sent_bt_{operator}_modelling\",\n",
    "    f\"e_n_host_{dpi_symbol}_modelling\",\n",
    ")\n",
    "n_dpi_days = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hosts = str(tuple(df_message.hosts.loc[lambda x: x != \"null_value\"].tolist()))\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "select all.rysecret\n",
    "from (  select phone_number rysecret, myq.host\n",
    "        from etl_fetch.{dpi_symbol}\n",
    "        lateral view explode(hosts) q as myq\n",
    "        where p_biz in ('jrunion')\n",
    "            AND p_date >= date_format(date_add(current_date(), -40), 'yyyyMMdd')\n",
    "            AND myq.host IN {hosts}\n",
    "      union all\n",
    "      select distinct rysecret, 1 host\n",
    "        from etl_swap.rp_biz_access_log\n",
    "        where p_biz in ('loan', 'credit', 'insurance') \n",
    "        and dt >= date_sub(current_date(), 180)\n",
    "    ) all\n",
    "join (select uid rysecret\n",
    "        from dw_resources.mapping_uid_property\n",
    "        where p_operate = {operator_id}\n",
    "        and p_province not in ('11')\n",
    "        ) pp on all.rysecret = pp.rysecret\n",
    "GROUP BY all.rysecret\n",
    "\"\"\"\n",
    ").createOrReplaceTempView(\"people_for_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label_data = spark.sql(\n",
    "    f\"\"\"\n",
    "SELECT \n",
    "\n",
    "    all.rysecret, \n",
    "    n_ins_host_10, n_ins_host_20, n_ins_host_30, fre_ins_host_10, fre_ins_host_20, fre_ins_host_30,\n",
    "    n_loan_host_10, n_loan_host_20, n_loan_host_30, fre_loan_host_10, fre_loan_host_20, fre_loan_host_30,\n",
    "    n_credit_host_10, n_credit_host_20, n_credit_host_30, fre_credit_host_10, fre_credit_host_20, fre_credit_host_30,\n",
    "    n_other_host_10, n_other_host_20, n_other_host_30,  fre_other_host_10, fre_other_host_20, fre_other_host_30,\n",
    "    ins_host_rate_10, ins_host_rate_20, ins_host_rate_30, ins_fre_rate_10, ins_fre_rate_20, ins_fre_rate_30,\n",
    "    loan_host_rate_10, loan_host_rate_20, loan_host_rate_30, loan_fre_rate_10, loan_fre_rate_20, loan_fre_rate_30,\n",
    "    credit_host_rate_10, credit_host_rate_20, credit_host_rate_30, credit_fre_rate_10, credit_fre_rate_20, credit_fre_rate_30,\n",
    "    n_ins_host_avg_30, n_ins_host_sd_30, n_ins_host_cv_30,\n",
    "    n_loan_host_avg_30, n_loan_host_sd_30, n_loan_host_cv_30,\n",
    "    n_credit_host_avg_30, n_credit_host_sd_30, n_credit_host_cv_30,\n",
    "    n_other_host_avg_30, n_other_host_sd_30, n_other_host_cv_30,\n",
    "    ins_host_rate_avg_30, ins_host_rate_sd_30, ins_host_rate_cv_30,\n",
    "    loan_host_rate_avg_30, loan_host_rate_sd_30, loan_host_rate_cv_30,\n",
    "    credit_host_rate_avg_30, credit_host_rate_sd_30, credit_host_rate_cv_30,\n",
    "    fre_ins_host_avg_30, fre_ins_host_sd_30, fre_ins_host_cv_30,\n",
    "    fre_loan_host_avg_30, fre_loan_host_sd_30, fre_loan_host_cv_30,\n",
    "    fre_credit_host_avg_30, fre_credit_host_sd_30, fre_credit_host_cv_30,\n",
    "    fre_other_host_avg_30, fre_other_host_sd_30, fre_other_host_cv_30,\n",
    "    ins_fre_rate_avg_30, ins_fre_rate_sd_30, ins_fre_rate_cv_30,\n",
    "    loan_fre_rate_avg_30, loan_fre_rate_sd_30, loan_fre_rate_cv_30,\n",
    "    credit_fre_rate_avg_30, credit_fre_rate_sd_30, credit_fre_rate_cv_30,\n",
    "    sent_30, sent_90, sent_180, days_since_sent_date,\n",
    "    click_30, click_90, click_180, days_since_click_date,\n",
    "    myl_rank, price, age, gender, maker, brand, new_age,\n",
    "    rank.city_code, province_code, city_level \n",
    "\n",
    "FROM people_for_prediction all\n",
    "LEFT JOIN (\n",
    "        SELECT *\n",
    "        FROM bigdata_insurance.{n_host}\n",
    "        WHERE update_date >= date_format(date_add(current_date(), -{n_dpi_days}), 'yyyyMMdd')\n",
    "        AND update_date < date_format(date_add(current_date(), -0), 'yyyyMMdd')\n",
    "        ) n_host on all.rysecret = n_host.rysecret\n",
    "LEFT JOIN (\n",
    "        SELECT \n",
    "            rysecret, update_date, sent_30, sent_90, sent_180, datediff(update_dt, sent_date) days_since_sent_date,\n",
    "            click_30, click_90, click_180, datediff(update_dt, click_date) days_since_click_date\n",
    "        FROM (\n",
    "            SELECT *, from_unixtime(unix_timestamp(cast(update_date as string), 'yyyyMMdd'), 'yyyy-MM-dd') update_dt\n",
    "            FROM bigdata_insurance.{n_sent}\n",
    "            ) sent    \n",
    "        WHERE update_date >= date_format(date_add(current_date(), -{n_dpi_days}), 'yyyyMMdd')\n",
    "            AND update_date < date_format(date_add(current_date(), -0), 'yyyyMMdd')\n",
    "        ) n_sent on all.rysecret = n_sent.rysecret\n",
    "LEFT JOIN (\n",
    "        SELECT rysecret, age, gender, price, maker\n",
    "        FROM bigdata_insurance.e_static_feature_table\n",
    "        ) st on all.rysecret = st.rysecret\n",
    "LEFT JOIN (\n",
    "        SELECT rysecret, brand\n",
    "        FROM bigdata_insurance.e_brand_table\n",
    "        ) bt on all.rysecret = bt.rysecret\n",
    "LEFT JOIN (\n",
    "        SELECT rysecret, first(cast(rank as int)) myl_rank\n",
    "        FROM (\n",
    "            SELECT uid rysecret, rank, p_date, max(p_date) over (partition by uid) max_date\n",
    "            FROM sample.e_mayilian) sample\n",
    "        WHERE p_date = max_date\n",
    "        GROUP BY rysecret\n",
    "        ) myl on all.rysecret = myl.rysecret\n",
    "LEFT JOIN (\n",
    "        SELECT uid rysecret, first(age) new_age\n",
    "        FROM model_dig.e_user_age_col\n",
    "        WHERE age is not null\n",
    "        GROUP BY rysecret\n",
    "        ) age on all.rysecret = age.rysecret\n",
    "JOIN (\n",
    "        SELECT uid rysecret, citycode city_code\n",
    "        FROM dw_resources.mapping_uid_property\n",
    "        ) city on all.rysecret = city.rysecret\n",
    "JOIN (\n",
    "        SELECT city_id city_code, province_id province_code, city_level_id city_level \n",
    "        FROM bigdata_insurance.e_citycode_rank_dict\n",
    "        ) rank on city.city_code = rank.city_code\n",
    "\"\"\"\n",
    ").cache()\n",
    "hosts = str(tuple(df_message.hosts.loc[lambda x: x != \"null_value\"].tolist()))\n",
    "dpi_data = spark.sql(\n",
    "    f\"\"\"\n",
    "SELECT \n",
    "    all.rysecret, n_visited.host,\n",
    "    n_10, n_20, n_30, n_avg_30, n_cv_30,\n",
    "    fre_10, fre_20, fre_30, fre_avg_30, fre_cv_30\n",
    "FROM people_for_prediction all\n",
    "JOIN (\n",
    "    SELECT *\n",
    "    FROM bigdata_insurance.{n_visited}\n",
    "    WHERE update_date >= date_format(date_add(current_date(), -{n_dpi_days}), 'yyyyMMdd')\n",
    "        AND update_date < date_format(date_add(current_date(), -0), 'yyyyMMdd') \n",
    "        AND host IN {hosts}\n",
    "    ) n_visited ON all.rysecret = n_visited.rysecret \n",
    "\"\"\"\n",
    ").cache()\n",
    "\n",
    "products_click = str(\n",
    "    tuple(df_message.products_click.loc[lambda x: x != \"null_value\"].tolist())\n",
    ")\n",
    "click_product_data = spark.sql(\n",
    "    f\"\"\"\n",
    "SELECT \n",
    "    all.rysecret, n_sent_bt.product,\n",
    "    click_30, click_90, click_180, click_360, fre_click_30, fre_click_90, fre_click_180, fre_click_360, days_since_click_date,\n",
    "    e_ip_30, e_ip_90, e_ip_180, e_ip_360, fre_e_ip_30, fre_e_ip_90, fre_e_ip_180, fre_e_ip_360, days_since_e_ip_date\n",
    "FROM people_for_prediction all\n",
    "JOIN (\n",
    "    SELECT *\n",
    "    FROM bigdata_insurance.{n_sent_bt}\n",
    "    WHERE update_date >= date_format(date_add(current_date(), -{n_dpi_days}), 'yyyyMMdd')\n",
    "        AND update_date < date_format(date_add(current_date(), -0), 'yyyyMMdd')\n",
    "        AND product IN {products_click} \n",
    "    ) n_sent_bt ON all.rysecret = n_sent_bt.rysecret\n",
    "\"\"\"\n",
    ").cache()\n",
    "\n",
    "product_sent = str(\n",
    "    tuple(df_message.products_send.loc[lambda x: x != \"null_value\"].tolist())\n",
    ")\n",
    "sent_product_data = spark.sql(\n",
    "    f\"\"\"\n",
    "SELECT \n",
    "    all.rysecret,  n_sent_bt.product,\n",
    "    sent_30, sent_90, sent_180, sent_360, days_since_sent_date,\n",
    "    call_30, call_90, call_180, call_360, days_since_call_date,\n",
    "    pick_30, pick_90, pick_180, pick_360, days_since_pick_date\n",
    "FROM people_for_prediction all\n",
    "JOIN (\n",
    "    SELECT *\n",
    "    FROM bigdata_insurance.{n_sent_bt}\n",
    "    WHERE update_date >= date_format(date_add(current_date(), -{n_dpi_days}), 'yyyyMMdd')\n",
    "        AND update_date < date_format(date_add(current_date(), -0), 'yyyyMMdd') \n",
    "        AND product IN {product_sent}\n",
    "    ) n_sent_bt ON all.rysecret = n_sent_bt.rysecret \n",
    "\"\"\"\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_data(df, df_message, df_col, df_message_col, label=False):\n",
    "    \"\"\"\n",
    "    df_col, df_message_col为list\n",
    "    \"\"\"\n",
    "    rename_dic = dict(zip(df_message_col, df_col))\n",
    "    fillna_dic = {\"rysecret\": \"A\", \"sent_date\": \"22220202\"}\n",
    "    if label:\n",
    "        fillna_dic[\"label\"] = 0\n",
    "    dummy_data = (\n",
    "        pd.concat(\n",
    "            [\n",
    "                pd.DataFrame(columns=df.columns),\n",
    "                (\n",
    "                    df_message[df_message_col]\n",
    "                    .replace(\"null_value\", np.nan)\n",
    "                    .dropna(how=\"all\")\n",
    "                    .fillna(method=\"ffill\")\n",
    "                    .rename(columns=rename_dic)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        .fillna(fillna_dic)\n",
    "        .fillna(-999)\n",
    "    )\n",
    "    return dummy_data\n",
    "\n",
    "\n",
    "label_data = (\n",
    "    label_data.fillna(\n",
    "        {\n",
    "            \"maker\": \"others\",\n",
    "            \"brand\": \"others\",\n",
    "            \"price\": -999,\n",
    "            \"age\": 0,\n",
    "            \"gender\": 2,\n",
    "            \"new_age\": \"null\",\n",
    "            \"myl_rank\": -999,\n",
    "        }\n",
    "    )\n",
    "    .fillna(\n",
    "        -999,\n",
    "        subset=[\n",
    "            ele\n",
    "            for ele in label_data.columns\n",
    "            if (\"_cv_\" in ele) or (\"days_since_\" in ele)\n",
    "        ],\n",
    "    )\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "dummy_dpi_data = get_dummy_data(dpi_data, df_message, [\"host\"], [\"hosts\"])\n",
    "dpi_data = (\n",
    "    spark.createDataFrame(dummy_dpi_data)\n",
    "    .union(dpi_data)\n",
    "    .filter(F.col(\"rysecret\") != \"A\")\n",
    "    .groupBy([\"rysecret\"])\n",
    "    .pivot(\"host\")\n",
    "    .agg(\n",
    "        *(\n",
    "            F.first(i).alias(i)\n",
    "            for i in dpi_data.columns\n",
    "            if i not in [\"rysecret\", \"host\"]\n",
    "        )\n",
    "    )\n",
    "    .fillna(0)\n",
    ")\n",
    "print(\"dpi_data\")\n",
    "\n",
    "dummy_click_product_data = get_dummy_data(\n",
    "    click_product_data, df_message, [\"product\"], [\"products_click\"]\n",
    ")\n",
    "click_product_data = (\n",
    "    spark.createDataFrame(dummy_click_product_data)\n",
    "    .union(click_product_data)\n",
    "    .filter(F.col(\"rysecret\") != \"A\")\n",
    "    .groupBy([\"rysecret\"])\n",
    "    .pivot(\"product\")\n",
    "    .agg(\n",
    "        *(\n",
    "            F.first(i).alias(i)\n",
    "            for i in click_product_data.columns\n",
    "            if i not in [\"rysecret\", \"product\"]\n",
    "        )\n",
    "    )\n",
    "    .fillna(0)\n",
    ")\n",
    "print(\"click_data\")\n",
    "\n",
    "dummy_sent_product_data = get_dummy_data(\n",
    "    sent_product_data, df_message, [\"product\"], [\"products_send\"]\n",
    ")\n",
    "sent_product_data = (\n",
    "    spark.createDataFrame(dummy_sent_product_data)\n",
    "    .union(sent_product_data)\n",
    "    .filter(F.col(\"rysecret\") != \"A\")\n",
    "    .groupBy([\"rysecret\"])\n",
    "    .pivot(\"product\")\n",
    "    .agg(\n",
    "        *(\n",
    "            F.first(i).alias(i)\n",
    "            for i in sent_product_data.columns\n",
    "            if i not in [\"rysecret\", \"product\"]\n",
    "        )\n",
    "    )\n",
    "    .fillna(0)\n",
    ")\n",
    "print(\"sent_data\")\n",
    "\n",
    "all_data = (\n",
    "    reduce(\n",
    "        lambda df1, df2: df1.join(df2, on=[\"rysecret\"], how=\"left\"),\n",
    "        [label_data, dpi_data, click_product_data, sent_product_data],\n",
    "    )\n",
    "    .withColumn(\"sent_date\", F.lit(\"20750101\"))\n",
    "    .withColumn(\"label\", F.lit(0))\n",
    ")\n",
    "print(\"all_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [i for i in fake_data.columns if i not in all_data.columns]:\n",
    "    all_data = all_data.withColumn(col, F.lit(0))\n",
    "\n",
    "all_data = (\n",
    "    all_data.select(fake_data.columns).fillna(-999).dropDuplicates(subset=[\"rysecret\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piped_all_data = pipeline_model.transform(all_data)\n",
    "\n",
    "data_pred = gbt_model.transform(piped_all_data)\n",
    "\n",
    "data_pred = data_pred.select(\"rysecret\", \"probability\", \"prediction\")\n",
    "\n",
    "data_pred.cache()\n",
    "\n",
    "data_pred.filter(F.col(\"prediction\") == 1).select(\"rysecret\").createOrReplaceTempView(\n",
    "    \"shot\"\n",
    ")\n",
    "\n",
    "operator_id2 = 8 if operator == \"cmcc\" else 9\n",
    "rule_name = f\"{operator}_dpi_{operator_id2}_dym_{product_name_ + '_' + model_type}_zhy\"\n",
    "print(rule_name)\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "set hive.exec.dynamic.partition.mode=nonstrict\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold_count(test_pre, prob_col=\"p_prob\"):\n",
    "    temp = []\n",
    "    for threshhold in np.arange(0, 1, 0.1):\n",
    "        num_p = test_pre.filter(F.col(prob_col) >= threshhold).count()\n",
    "        temp.append((threshhold.round(2), num_p))\n",
    "    return pd.DataFrame(temp, columns=[\"prob\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_to_columns = F.udf(lambda x: x[1].item(), DoubleType())\n",
    "data_pred1 = data_pred.withColumn(\"p_prob\", vec_to_columns(\"probability\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_threshold_count(data_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    data_pred.withColumn(\"p_prob\", vec_to_columns(\"probability\"))\n",
    "    .filter(F.col(\"p_prob\") >= 0.2)\n",
    "    .select(\"rysecret\")\n",
    "    .createOrReplaceTempView(\"shot\")\n",
    ")\n",
    "operator_id2 = 8 if operator == \"cmcc\" else 9\n",
    "rule_name = f\"{operator}_dpi_{operator_id2}_{product_name_ + '_' + 'melo' + '_' + model_type}_zhy\"\n",
    "print(rule_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "INSERT OVERWRITE table bigdata_insurance.e_zhuyan_bx_rules_pre partition(rule_name) \n",
    "SELECT DISTINCT a.rysecret, b.citycode, b.operator,  b.suffix, b.province, \n",
    "        date_add(current_date(), 1) AS p_date, '{rule_name}' AS rule_name\n",
    "FROM shot AS a \n",
    "JOIN dw_resources.mapping_uid_property AS b ON a.rysecret = b.uid \n",
    "LEFT JOIN (select rysecret \n",
    "            from bigdata_insurance.e_bt_black_list_bx \n",
    "            where level = 'jdjk_waihu') AS c ON a.rysecret = c.rysecret \n",
    "WHERE c.rysecret is NULL AND b.operator = {operator_id}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "select count(1) \n",
    "from bigdata_insurance.e_zhuyan_bx_rules_pre\n",
    "where rule_name = '{rule_name}'\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
